@article{wendler2024llamas,
  title={Do Llamas Work in English? On the Latent Language of Multilingual Transformers},
  author={Wendler, Chris and Veselovsky, Veniamin and Monea, Giovanni and West, Robert},
  journal={arXiv preprint arXiv:2402.10588},
  year={2024}
}

@article{zhang2023dont,
  title={Don't Trust {ChatGPT} when your Question is not in English: A Study of Multilingual Abilities and Types of {LLM}s},
  author={Zhang, Xiang and Li, Senyu and Hauer, Bradley and Shi, Ning and Kondrak, Grzegorz},
  journal={arXiv preprint arXiv:2305.16339},
  year={2023}
}

@article{liu2024translation,
  title={Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models},
  author={Liu, Chaoqun and Zhang, Wenxuan and Zhao, Yiran and Luu, Anh Tuan and Bing, Lidong},
  journal={arXiv preprint arXiv:2403.10258},
  year={2024}
}

@inproceedings{hu2020xtreme,
  title={{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation},
  author={Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  booktitle={International Conference on Machine Learning},
  pages={4411--4421},
  year={2020},
  organization={PMLR}
}

@article{adelani2023sib200,
  title={{SIB-200}: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects},
  author={Adelani, David Ifeoluwa and Liu, Hannah and Shen, Xiaoyu and Schwenk, Holger and Neubig, Graham},
  journal={arXiv preprint arXiv:2309.07445},
  year={2023}
}

@article{asai2023buffet,
  title={{BUFFET}: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer},
  author={Asai, Akari and Kudugunta, Sneha and Yu, Xinyan Velocity and Blevins, Terra and Gonen, Hila and Fitzgerald, Machel and Neubig, Graham and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14857},
  year={2023}
}

@article{conneau2019xlmr,
  title={{Unsupervised Cross-lingual Representation Learning at Scale}},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}

@inproceedings{conneau2018xnli,
  title={{XNLI}: Evaluating Cross-lingual Sentence Representations},
  author={Conneau, Alexis and Rinott, Rupert and Lample, Guillaume and Williams, Adina and Bowman, Samuel and Schwenk, Holger and Stoyanov, Veselin},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2475--2485},
  year={2018}
}

@inproceedings{williams2018mnli,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1112--1122},
  year={2018}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{openai2023gpt4,
  title={{GPT-4} Technical Report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={{LLaMA}: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{shi2022language,
  title={Language Models are Multilingual Chain-of-Thought Reasoners},
  author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.03057},
  year={2022}
}

@article{workshop2022bloom,
  title={{BLOOM}: A 176B-Parameter Open-Access Multilingual Language Model},
  author={{BigScience Workshop}},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{xue2021mt5,
  title={{mT5}: A Massively Multilingual Pre-trained Text-to-Text Transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  journal={arXiv preprint arXiv:2010.11934},
  year={2021}
}

@inproceedings{devlin2019bert,
  title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4171--4186},
  year={2019}
}

@article{pires2019multilingual,
  title={How Multilingual is Multilingual {BERT}?},
  author={Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  journal={arXiv preprint arXiv:1906.01502},
  year={2019}
}
