\begin{abstract}
Large Language Models (LLMs) are increasingly deployed globally, yet their training data remains predominantly English. This raises fundamental questions about cross-lingual performance equity and whether these models implicitly rely on English-centric processing pathways. We present a systematic evaluation of two state-of-the-art models---GPT-4.1 and Claude Sonnet 4.5---on the XNLI benchmark across 10 languages spanning 6 language families. Our experiments reveal significant English-centric bias in both models: Claude exhibits a 7.25\% average performance drop from English to other languages compared to 3.56\% for GPT-4.1. Critically, we demonstrate that a translate-to-English approach yields dramatic improvements for non-Latin script languages in Claude (up to +14.7\% for Chinese and Arabic), while providing minimal benefit for European languages. This asymmetric translate-test effect provides behavioral evidence that models may internally process non-English inputs through English-centric pathways. We further identify systematic performance patterns across language families, with Indo-European languages consistently outperforming Sino-Tibetan, Afro-Asiatic, and Niger-Congo families. Our findings have practical implications for multilingual deployment and underscore the need for more balanced training approaches in LLM development.
\end{abstract}
