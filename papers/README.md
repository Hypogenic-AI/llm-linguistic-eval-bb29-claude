# Downloaded Papers

This directory contains academic papers relevant to evaluating linguistic performance in LLMs.

## Paper List

### 1. Do Llamas Work in English? On the Latent Language of Multilingual Transformers
- **File**: `2402.10588_llamas_english_pivot.pdf`
- **Authors**: Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West
- **Year**: 2024
- **arXiv**: https://arxiv.org/abs/2402.10588
- **Why relevant**: Empirical investigation of English as internal pivot language in LLMs using logit lens analysis. Core paper for understanding implicit translation mechanisms.

### 2. Don't Trust ChatGPT when your Question is not in English
- **File**: `2305.16339_dont_trust_chatgpt_non_english.pdf`
- **Authors**: Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, Grzegorz Kondrak
- **Year**: 2023
- **arXiv**: https://arxiv.org/abs/2305.16339
- **Why relevant**: Systematic framework for evaluating multilingual LLM capabilities, introduces bilingualism typology for LLMs.

### 3. Is Translation All You Need?
- **File**: `2403.10258_translation_multilingual.pdf`
- **Authors**: Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, et al.
- **Year**: 2024
- **arXiv**: https://arxiv.org/abs/2403.10258
- **Why relevant**: Comprehensive evaluation of translation strategies for multilingual tasks, tests translate-to-English approach.

### 4. XTREME: A Massively Multilingual Multi-task Benchmark
- **File**: `2003.11080_xtreme_benchmark.pdf`
- **Authors**: Junjie Hu, Sebastian Ruder, Aditya Siddhant, et al.
- **Year**: 2020
- **arXiv**: https://arxiv.org/abs/2003.11080
- **Why relevant**: Foundational benchmark for cross-lingual evaluation, covers 40 languages and 9 tasks.

### 5. SIB-200: Topic Classification in 200+ Languages
- **File**: `2309.07445_sib200.pdf`
- **Authors**: David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, et al.
- **Year**: 2023
- **arXiv**: https://arxiv.org/abs/2309.07445
- **Why relevant**: Largest multilingual NLU benchmark (205 languages), good for testing low-resource language hypothesis.

### 6. BUFFET: Benchmarking LLMs for Few-shot Cross-lingual Transfer
- **File**: `2305.14857_buffet.pdf`
- **Authors**: Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, et al.
- **Year**: 2023
- **arXiv**: https://arxiv.org/abs/2305.14857
- **Why relevant**: Few-shot cross-lingual benchmark, 15 tasks across 54 languages.

### 7. XLM-RoBERTa: Unsupervised Cross-lingual Representation Learning at Scale
- **File**: `1911.02116_xlm_roberta.pdf`
- **Authors**: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, et al.
- **Year**: 2019
- **arXiv**: https://arxiv.org/abs/1911.02116
- **Why relevant**: Foundational multilingual pre-trained model, strong baseline for cross-lingual tasks.

### 8. A Survey on Multilingual Large Language Models
- **File**: `2403.03887_multilingual_llm_survey.pdf`
- **Authors**: Yuemei Xu, Ling Hu, Jiayi Zhao, et al.
- **Year**: 2024
- **arXiv**: https://arxiv.org/abs/2403.03887
- **Why relevant**: Comprehensive survey on MLLMs covering corpora, alignment, and bias.

### 9. MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes
- **File**: `2406.18902_mbbq_crosslingual_bias.pdf`
- **Authors**: Vera Neplenbroek, Arianna Bisazza, R. Fernandez
- **Year**: 2024
- **arXiv**: https://arxiv.org/abs/2406.18902
- **Why relevant**: Evaluates bias across languages, shows safety fine-tuning effects differ by language.

### 10. MEXA: Multilingual Evaluation of English-Centric LLMs
- **File**: `2407.02797_mexa_multilingual.pdf`
- **Authors**: Amir Hossein Kargaran, Ali Modarressi, et al.
- **Year**: 2024
- **arXiv**: https://arxiv.org/abs/2407.02797
- **Why relevant**: Directly evaluates English-centric LLMs via cross-lingual alignment.

## Total Papers: 10
